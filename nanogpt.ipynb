{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/steelannelida/nanoGPT/blob/master/nanogpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtaaN1C1BT8P",
        "outputId": "77680f87-314e-4e98-aae5-cd626fb969ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nanoGPT'...\n",
            "remote: Enumerating objects: 703, done.\u001b[K\n",
            "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 703 (delta 11), reused 17 (delta 8), pack-reused 682 (from 1)\u001b[K\n",
            "Receiving objects: 100% (703/703), 1.10 MiB | 8.36 MiB/s, done.\n",
            "Resolving deltas: 100% (396/396), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/steelannelida/nanoGPT.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vq-zNSZYCPTL",
        "outputId": "5d63a890-a9b4-4979-90d9-2a6037fef785"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Collecting triton\n",
            "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec (from torch)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.17.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, triton, fsspec, dill, tiktoken, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 tiktoken-0.8.0 triton-3.1.0 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch numpy transformers datasets tiktoken wandb tqdm triton\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSPqPSIgCAGI",
        "outputId": "98b5258d-13e9-4a11-c878-5169b8a0da98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train has 301,966 tokens\n",
            "val has 36,059 tokens\n"
          ]
        }
      ],
      "source": [
        "!yes  | python nanoGPT/data/shakespeare/prepare.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vziHkeCMFLt8"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CDskvzaJC2dL",
        "outputId": "24b01ecf-4b0b-4b13-c759-7a9760a477bd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sandwitch'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "with open('nanoGPT/data/shakespeare/input.txt') as f:\n",
        "  text = f.read()\n",
        "\n",
        "chars = sorted(set(text))\n",
        "vocab_size=len(chars)\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "\n",
        "def encode(t):\n",
        "  return [stoi[c] for c in t]\n",
        "\n",
        "def decode(seq):\n",
        "  return ''.join([chars[i] for i in seq])\n",
        "\n",
        "decode(encode(\"sandwitch\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "emtQzoojEXUI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "data = torch.tensor(encode(text), device=device)\n",
        "data.shape, data.dtype\n",
        "\n",
        "n = int(data.shape[0] * 0.9)\n",
        "train_data = data[:n]\n",
        "valid_data = data[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3O_mdTCJyKW",
        "outputId": "d6828963-0800-4142-aeb5-8f879d5ccc33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " in person bear me\n",
            "Like a true friar. More reasons for this action\n",
            "At our more leisure shall I render you;\n",
            "Only, this one: Lord \n",
            "in person bear me\n",
            "Like a true friar. More reasons for this action\n",
            "At our more leisure shall I render you;\n",
            "Only, this one: Lord A\n"
          ]
        }
      ],
      "source": [
        "#torch.manual_seed(1337)\n",
        "\n",
        "\n",
        "def get_batch(data_set=train_data, batch_size=32, seq_length=128):\n",
        "    x = torch.zeros([batch_size, seq_length], dtype=torch.int, device=device)\n",
        "    y = torch.zeros([batch_size, seq_length], dtype=torch.int, device=device)\n",
        "    for b in range(batch_size):\n",
        "        t = torch.randint(0, data_set.shape[0] - seq_length - 1, [1])\n",
        "        x[b] = data_set[t:t+seq_length]\n",
        "        y[b] = data_set[t+1:t+seq_length+1]\n",
        "    return x, y\n",
        "\n",
        "x, y = get_batch()\n",
        "print(decode(x[13]))\n",
        "print(decode(y[13]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Q8L2PQYERYsM",
        "outputId": "16cafab3-5bb9-46d2-ba58-a34c3f9c13ea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'51241+5341+61932=118514'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import random\n",
        "arith_chars = '0123456789-=*+'\n",
        "arith_idx = {c:idx for idx, c in enumerate(arith_chars)}\n",
        "\n",
        "\n",
        "def gen_expression(digits=5, terms=3):\n",
        "  s = ''\n",
        "  for i in range(terms):\n",
        "    if i > 0:\n",
        "      s += random.choice('*-+')\n",
        "    num = ''\n",
        "    for j in range(digits):\n",
        "      num += random.choice('0123456789')\n",
        "    num = str(int(num))\n",
        "    s += num\n",
        "  return s\n",
        "\n",
        "def gen_eq():\n",
        "  expr = gen_expression()\n",
        "  value = eval(expr)\n",
        "  return f'{expr}={value}'\n",
        "\n",
        "gen_eq()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qdTNUaLJpyc",
        "outputId": "d74ddded-2c68-402d-d969-279c0f709a97"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "d6Huvb7cNXcN",
        "outputId": "a22ba5bc-9fd7-440e-85e4-1544aee59f37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-e743fabf5d0e>:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  idx = torch.tensor(idx, device=device)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hello$!nOiRLr.k'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self, embed_size=256, wide_size=1024, nheads=16, dropout=0.5):\n",
        "    super().__init__()\n",
        "    # self.attn = nn.MultiheadAttention(embed_size, nheads,\n",
        "    #                                   batch_first=True)\n",
        "    self.nheads = nheads\n",
        "    self.head_size = embed_size // nheads\n",
        "    self.pre_attn = nn.Linear(embed_size, 3 * embed_size)\n",
        "    self.post_attn = nn.Linear(embed_size, embed_size)\n",
        "    self.ln1 = nn.LayerNorm(embed_size)\n",
        "    self.ln2 = nn.LayerNorm(embed_size)\n",
        "    self.lin1 = nn.Linear(embed_size, wide_size)\n",
        "    self.lin2 = nn.Linear(wide_size, embed_size)\n",
        "    self.drop = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    N, L, C = x.shape\n",
        "    x = self.ln1(x)\n",
        "    #a, w = self.attn.forward(x, x, x, attn_mask=mask, is_causal=True)\n",
        "    kqv = self.pre_attn(x).view(N, L, self.nheads, 3*self.head_size).permute(0, 2, 1, 3)\n",
        "    k = kqv[:,:,:,:self.head_size]\n",
        "    q = kqv[:,:,:,self.head_size : 2*self.head_size]\n",
        "    v = kqv[:,:,:,2*self.head_size:]\n",
        "    av = nn.functional.scaled_dot_product_attention(q, k, v, mask.view(1, L, L))\n",
        "    a = self.post_attn(av.permute(0, 2, 1, 3).view(N, L, C))\n",
        "    x = x + self.drop(a)\n",
        "    x = self.ln2(x)\n",
        "    xx = self.lin1(x)\n",
        "    xx = nn.functional.gelu(xx)\n",
        "    y = self.lin2(xx)\n",
        "    return x + y\n",
        "\n",
        "\n",
        "\n",
        "class LM(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_size=256, nheads=16, max_pos=2048, num_layers=3, dropout=0.5):\n",
        "    super().__init__()\n",
        "    self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
        "    self.pos_embeddings = nn.Embedding(max_pos, embed_size)\n",
        "    self.layers = [\n",
        "        DecoderLayer(embed_size=embed_size, wide_size=4 * embed_size,\n",
        "                     nheads=nheads, dropout=dropout)\n",
        "        for _ in range(num_layers)\n",
        "    ]\n",
        "    for i, layer in enumerate(self.layers):\n",
        "      self.add_module(f'decoder-{i}', layer)\n",
        "    self.ln = nn.LayerNorm(embed_size)\n",
        "    self.out = nn.Linear(embed_size, vocab_size)\n",
        "    self.max_pos = max_pos\n",
        "    self.to(device)\n",
        "\n",
        "  def forward(self, idx):\n",
        "    idx = torch.tensor(idx, device=device)\n",
        "    l = idx.shape[-1]\n",
        "    pe = self.pos_embeddings(torch.arange(0, l, device=device))\n",
        "    e = self.embeddings(idx)\n",
        "    e = e + pe.view(1, *pe.shape)\n",
        "    mask = torch.tril(torch.ones([l, l], dtype=torch.bool, device=device))\n",
        "    #print(mask)\n",
        "    #print(w)\n",
        "    for layer in self.layers:\n",
        "      e = layer(e, mask)\n",
        "    logits = self.out.forward(self.ln(e))\n",
        "    return logits\n",
        "\n",
        "  def generate(self, prompt, l):\n",
        "    prompt = torch.tensor(prompt)\n",
        "    pl = prompt.shape[0]\n",
        "    result = torch.zeros([pl + l], dtype=torch.int, device=device)\n",
        "    result[:pl] = prompt\n",
        "    for i in range(l):\n",
        "      logits = self.forward(result[:pl + i])\n",
        "      sm = logits[:,-1].flatten().softmax(0)\n",
        "      next_idx = torch.multinomial(sm, 1)\n",
        "      result[i+pl] = next_idx\n",
        "    return result\n",
        "\n",
        "model = LM(vocab_size)\n",
        "\n",
        "k=x\n",
        "q=x[:,:-1]\n",
        "model.forward(encode('jello'))\n",
        "decode(model.generate(encode('hello'), 10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmqqz6egO50w",
        "outputId": "24038d41-54b0-4ed3-f6b1-3a998f41e94c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-bbf3a1f534d0>:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  idx = torch.tensor(idx, device=device)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.3758, device='cuda:0', grad_fn=<NllLoss2DBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "x, y = get_batch()\n",
        "logits = model(x)\n",
        "loss_fun = nn.CrossEntropyLoss()\n",
        "loss = loss_fun(logits.permute(0, 2, 1), y.long())\n",
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nkmynipacvK",
        "outputId": "9e7dc65a-74a9-432c-b320-e42bc977eeee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-48-b8b286eeb8b6>:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "model = LM(vocab_size, num_layers=8)\n",
        "model = torch.compile(model)\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "xv, yv = get_batch(valid_data, batch_size=256)\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5yC7g_TUpZz",
        "outputId": "933a7cd4-bf3f-4293-dc3b-49a98ab00cf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\t2.330084\t2.321952\n",
            "10\t2.240674\t2.292088\n",
            "20\t2.237633\t2.282294\n",
            "30\t2.224840\t2.262885\n",
            "40\t2.251450\t2.256018\n",
            "50\t2.207198\t2.234732\n",
            "60\t2.207946\t2.214935\n",
            "70\t2.149199\t2.189362\n",
            "80\t2.131125\t2.168802\n",
            "90\t2.147406\t2.160749\n",
            "100\t2.092362\t2.128591\n",
            "110\t2.067341\t2.116857\n",
            "120\t2.055706\t2.110833\n",
            "130\t2.046234\t2.099337\n",
            "140\t2.004681\t2.089618\n",
            "150\t2.047931\t2.085074\n",
            "160\t2.035627\t2.087979\n",
            "170\t1.977736\t2.072329\n",
            "180\t2.005031\t2.046731\n",
            "190\t1.968908\t2.029851\n",
            "200\t1.973555\t2.017879\n",
            "210\t1.946047\t2.016523\n",
            "220\t1.918499\t2.008947\n",
            "230\t1.878981\t2.003235\n",
            "240\t1.906580\t1.993696\n",
            "250\t1.901069\t1.983843\n",
            "260\t1.886767\t1.956607\n",
            "270\t1.860534\t1.964939\n",
            "280\t1.843090\t1.956652\n",
            "290\t1.867823\t1.941780\n",
            "300\t1.785640\t1.946332\n",
            "310\t1.789663\t1.940130\n",
            "320\t1.835106\t1.942779\n",
            "330\t1.833072\t1.935559\n",
            "340\t1.858463\t1.915943\n",
            "350\t1.798697\t1.925139\n",
            "360\t1.774139\t1.904985\n",
            "370\t1.756099\t1.887990\n",
            "380\t1.807185\t1.892697\n",
            "390\t1.772618\t1.887367\n",
            "400\t1.780906\t1.880911\n",
            "410\t1.778962\t1.878105\n",
            "420\t1.731223\t1.875539\n",
            "430\t1.740290\t1.867718\n",
            "440\t1.700108\t1.851022\n",
            "450\t1.678572\t1.850341\n",
            "460\t1.739072\t1.842906\n",
            "470\t1.725583\t1.836615\n",
            "480\t1.724099\t1.845148\n",
            "490\t1.696232\t1.834029\n",
            "500\t1.733702\t1.835828\n",
            "510\t1.655089\t1.822899\n",
            "520\t1.658494\t1.833305\n",
            "530\t1.732943\t1.822280\n",
            "540\t1.682128\t1.811879\n",
            "550\t1.696227\t1.793616\n",
            "560\t1.709701\t1.793206\n",
            "570\t1.696005\t1.786273\n",
            "580\t1.674744\t1.789193\n",
            "590\t1.646926\t1.776546\n",
            "600\t1.678167\t1.774856\n",
            "610\t1.616143\t1.779051\n",
            "620\t1.667219\t1.775939\n",
            "630\t1.638324\t1.772197\n",
            "640\t1.692570\t1.769481\n",
            "650\t1.575279\t1.769734\n",
            "660\t1.632186\t1.766814\n",
            "670\t1.682282\t1.754682\n",
            "680\t1.653976\t1.755342\n",
            "690\t1.595718\t1.750313\n",
            "700\t1.567598\t1.754197\n",
            "710\t1.593277\t1.754003\n",
            "720\t1.589450\t1.747524\n",
            "730\t1.609767\t1.741648\n",
            "740\t1.521253\t1.734478\n",
            "750\t1.590411\t1.728021\n",
            "760\t1.592423\t1.726242\n",
            "770\t1.568123\t1.717073\n",
            "780\t1.532432\t1.718520\n",
            "790\t1.574404\t1.719414\n",
            "800\t1.517330\t1.729975\n",
            "810\t1.582823\t1.723300\n",
            "820\t1.590047\t1.711194\n",
            "830\t1.569043\t1.708952\n",
            "840\t1.632895\t1.698683\n",
            "850\t1.539399\t1.703685\n",
            "860\t1.615383\t1.704374\n",
            "870\t1.543400\t1.710850\n",
            "880\t1.515830\t1.712911\n",
            "890\t1.556333\t1.700240\n",
            "900\t1.475357\t1.688731\n",
            "910\t1.535797\t1.694503\n",
            "920\t1.563280\t1.694556\n",
            "930\t1.577556\t1.698141\n",
            "940\t1.575097\t1.695077\n",
            "950\t1.576076\t1.690426\n",
            "960\t1.544244\t1.693554\n",
            "970\t1.594498\t1.688141\n",
            "980\t1.525938\t1.690688\n",
            "990\t1.529382\t1.681751\n",
            "1000\t1.534625\t1.682999\n",
            "1010\t1.521595\t1.682981\n",
            "1020\t1.530179\t1.682420\n",
            "1030\t1.516141\t1.676953\n",
            "1040\t1.515410\t1.670659\n",
            "1050\t1.518321\t1.680816\n",
            "1060\t1.482830\t1.675861\n",
            "1070\t1.521675\t1.675472\n",
            "1080\t1.506668\t1.666137\n",
            "1090\t1.492330\t1.671607\n",
            "1100\t1.514523\t1.663479\n",
            "1110\t1.446882\t1.664504\n",
            "1120\t1.468194\t1.671817\n",
            "1130\t1.474164\t1.653145\n",
            "1140\t1.504883\t1.667863\n",
            "1150\t1.488824\t1.662047\n",
            "1160\t1.494241\t1.654132\n",
            "1170\t1.476135\t1.655408\n",
            "1180\t1.420246\t1.648975\n",
            "1190\t1.464659\t1.647273\n",
            "1200\t1.509103\t1.641555\n",
            "1210\t1.479459\t1.660908\n",
            "1220\t1.439918\t1.644542\n",
            "1230\t1.516832\t1.639385\n",
            "1240\t1.432951\t1.635759\n",
            "1250\t1.442268\t1.641173\n",
            "1260\t1.455310\t1.633697\n",
            "1270\t1.575547\t1.629399\n",
            "1280\t1.476212\t1.624851\n",
            "1290\t1.494285\t1.632891\n",
            "1300\t1.435507\t1.618773\n",
            "1310\t1.458812\t1.633771\n",
            "1320\t1.462206\t1.625583\n",
            "1330\t1.487592\t1.626727\n",
            "1340\t1.492394\t1.629267\n",
            "1350\t1.416130\t1.626681\n",
            "1360\t1.445445\t1.625748\n",
            "1370\t1.429134\t1.620595\n",
            "1380\t1.491001\t1.622565\n",
            "1390\t1.485698\t1.616090\n",
            "1400\t1.450664\t1.619781\n",
            "1410\t1.438313\t1.613171\n",
            "1420\t1.468749\t1.614659\n",
            "1430\t1.421837\t1.608071\n",
            "1440\t1.504573\t1.618802\n",
            "1450\t1.400735\t1.608666\n",
            "1460\t1.444085\t1.612025\n",
            "1470\t1.435135\t1.621308\n",
            "1480\t1.427376\t1.605644\n",
            "1490\t1.443640\t1.603262\n",
            "1500\t1.430711\t1.601516\n",
            "1510\t1.420185\t1.598258\n",
            "1520\t1.483459\t1.598284\n",
            "1530\t1.430356\t1.612955\n",
            "1540\t1.450252\t1.605983\n",
            "1550\t1.450344\t1.609038\n",
            "1560\t1.347047\t1.601969\n",
            "1570\t1.383247\t1.591708\n",
            "1580\t1.461308\t1.591371\n",
            "1590\t1.409986\t1.597978\n",
            "1600\t1.429142\t1.601241\n",
            "1610\t1.445248\t1.590703\n",
            "1620\t1.433280\t1.592780\n",
            "1630\t1.422821\t1.590439\n",
            "1640\t1.421463\t1.593295\n",
            "1650\t1.480858\t1.591614\n",
            "1660\t1.380295\t1.592465\n",
            "1670\t1.403776\t1.582859\n",
            "1680\t1.411875\t1.586322\n",
            "1690\t1.407345\t1.589257\n",
            "1700\t1.424009\t1.601903\n",
            "1710\t1.382698\t1.594729\n",
            "1720\t1.358517\t1.597674\n",
            "1730\t1.346095\t1.594715\n",
            "1740\t1.420296\t1.590901\n",
            "1750\t1.423925\t1.587735\n",
            "1760\t1.399160\t1.578598\n",
            "1770\t1.353640\t1.580585\n",
            "1780\t1.421683\t1.579407\n",
            "1790\t1.488520\t1.583825\n",
            "1800\t1.351158\t1.590084\n",
            "1810\t1.402535\t1.578232\n",
            "1820\t1.362828\t1.583983\n",
            "1830\t1.411599\t1.588171\n",
            "1840\t1.418759\t1.575510\n",
            "1850\t1.347428\t1.578603\n",
            "1860\t1.406708\t1.576853\n",
            "1870\t1.380720\t1.581891\n",
            "1880\t1.394036\t1.580385\n",
            "1890\t1.403557\t1.587946\n",
            "1900\t1.409878\t1.578980\n",
            "1910\t1.395688\t1.572900\n",
            "1920\t1.393598\t1.566253\n",
            "1930\t1.404587\t1.568672\n",
            "1940\t1.350529\t1.579326\n",
            "1950\t1.438676\t1.580944\n",
            "1960\t1.374472\t1.574814\n",
            "1970\t1.454352\t1.565161\n",
            "1980\t1.377658\t1.573534\n",
            "1990\t1.324630\t1.568696\n",
            "2000\t1.428740\t1.561830\n",
            "2010\t1.360744\t1.568181\n",
            "2020\t1.385634\t1.562521\n",
            "2030\t1.439627\t1.566668\n",
            "2040\t1.287860\t1.569495\n",
            "2050\t1.393231\t1.577400\n",
            "2060\t1.338841\t1.566440\n",
            "2070\t1.373347\t1.573147\n",
            "2080\t1.378061\t1.566078\n",
            "2090\t1.368273\t1.564959\n",
            "2100\t1.342482\t1.557891\n",
            "2110\t1.356666\t1.565461\n",
            "2120\t1.385123\t1.566717\n",
            "2130\t1.352643\t1.556019\n",
            "2140\t1.376062\t1.559887\n",
            "2150\t1.390462\t1.556512\n",
            "2160\t1.405225\t1.548083\n",
            "2170\t1.396489\t1.548613\n",
            "2180\t1.364232\t1.554259\n",
            "2190\t1.373950\t1.556809\n",
            "2200\t1.338778\t1.556597\n",
            "2210\t1.384049\t1.564669\n",
            "2220\t1.348270\t1.555339\n",
            "2230\t1.343519\t1.554256\n",
            "2240\t1.357306\t1.558515\n",
            "2250\t1.411971\t1.557389\n",
            "2260\t1.406074\t1.546513\n",
            "2270\t1.291937\t1.545620\n",
            "2280\t1.354890\t1.548316\n",
            "2290\t1.306981\t1.550593\n",
            "2300\t1.343204\t1.561296\n",
            "2310\t1.361828\t1.555223\n",
            "2320\t1.350288\t1.550025\n",
            "2330\t1.364272\t1.551541\n",
            "2340\t1.384881\t1.550056\n",
            "2350\t1.335177\t1.547961\n",
            "2360\t1.298833\t1.552646\n",
            "2370\t1.338954\t1.555147\n",
            "2380\t1.401840\t1.556637\n",
            "2390\t1.342817\t1.550129\n",
            "2400\t1.345767\t1.543076\n",
            "2410\t1.344695\t1.537365\n",
            "2420\t1.323637\t1.539086\n",
            "2430\t1.337065\t1.540044\n",
            "2440\t1.362094\t1.545170\n",
            "2450\t1.289703\t1.544190\n",
            "2460\t1.392316\t1.542454\n",
            "2470\t1.324255\t1.529914\n",
            "2480\t1.340666\t1.537381\n",
            "2490\t1.326493\t1.547118\n",
            "2500\t1.360627\t1.542072\n",
            "2510\t1.314208\t1.537908\n",
            "2520\t1.370000\t1.532670\n",
            "2530\t1.282073\t1.544837\n",
            "2540\t1.395141\t1.542609\n",
            "2550\t1.292906\t1.537465\n",
            "2560\t1.362008\t1.543784\n",
            "2570\t1.318971\t1.537825\n",
            "2580\t1.327689\t1.535232\n",
            "2590\t1.316115\t1.535027\n",
            "2600\t1.390167\t1.540513\n",
            "2610\t1.314071\t1.536190\n",
            "2620\t1.358466\t1.535204\n",
            "2630\t1.342565\t1.535134\n",
            "2640\t1.262377\t1.543407\n",
            "2650\t1.291411\t1.536825\n",
            "2660\t1.292138\t1.542961\n",
            "2670\t1.304502\t1.552995\n",
            "2680\t1.400502\t1.544833\n",
            "2690\t1.319161\t1.523628\n",
            "2700\t1.324158\t1.526772\n",
            "2710\t1.310466\t1.529774\n",
            "2720\t1.310340\t1.529482\n",
            "2730\t1.321885\t1.532517\n",
            "2740\t1.305144\t1.534266\n",
            "2750\t1.362445\t1.517129\n",
            "2760\t1.298451\t1.532457\n",
            "2770\t1.355552\t1.524849\n",
            "2780\t1.350626\t1.527755\n",
            "2790\t1.268154\t1.516091\n",
            "2800\t1.310792\t1.519086\n",
            "2810\t1.344823\t1.525139\n",
            "2820\t1.338197\t1.513342\n",
            "2830\t1.274464\t1.525405\n",
            "2840\t1.259990\t1.519696\n",
            "2850\t1.298587\t1.514445\n",
            "2860\t1.333066\t1.518965\n",
            "2870\t1.314499\t1.521911\n",
            "2880\t1.379014\t1.519582\n",
            "2890\t1.304007\t1.526994\n",
            "2900\t1.333013\t1.515248\n",
            "2910\t1.321537\t1.534482\n",
            "2920\t1.277066\t1.528744\n",
            "2930\t1.327830\t1.519410\n",
            "2940\t1.315109\t1.517496\n",
            "2950\t1.369437\t1.517659\n",
            "2960\t1.304938\t1.521783\n",
            "2970\t1.301896\t1.525720\n",
            "2980\t1.297453\t1.515493\n",
            "2990\t1.321761\t1.521178\n",
            "3000\t1.299120\t1.507096\n",
            "3010\t1.280100\t1.504335\n",
            "3020\t1.302005\t1.518672\n",
            "3030\t1.331300\t1.514439\n",
            "3040\t1.343384\t1.516961\n",
            "3050\t1.334610\t1.511844\n",
            "3060\t1.253301\t1.515494\n",
            "3070\t1.310514\t1.525056\n",
            "3080\t1.319940\t1.510489\n",
            "3090\t1.270066\t1.519365\n",
            "3100\t1.300703\t1.518049\n",
            "3110\t1.360593\t1.518744\n",
            "3120\t1.238532\t1.515272\n",
            "3130\t1.293417\t1.514903\n",
            "3140\t1.266210\t1.517333\n",
            "3150\t1.313337\t1.520162\n",
            "3160\t1.300810\t1.515803\n",
            "3170\t1.300312\t1.523960\n",
            "3180\t1.291697\t1.515538\n",
            "3190\t1.340659\t1.513297\n",
            "3200\t1.242136\t1.510107\n",
            "3210\t1.278135\t1.517118\n",
            "3220\t1.313402\t1.524219\n",
            "3230\t1.257956\t1.526018\n",
            "3240\t1.296737\t1.526060\n",
            "3250\t1.273364\t1.522499\n",
            "3260\t1.260764\t1.524642\n",
            "3270\t1.281632\t1.520710\n",
            "3280\t1.288943\t1.515212\n",
            "3290\t1.323376\t1.522377\n",
            "3300\t1.305540\t1.517319\n",
            "3310\t1.279892\t1.511767\n",
            "3320\t1.272080\t1.514594\n",
            "3330\t1.316058\t1.514686\n",
            "3340\t1.301377\t1.522550\n",
            "3350\t1.285442\t1.520834\n",
            "3360\t1.236347\t1.528610\n",
            "3370\t1.269482\t1.515563\n",
            "3380\t1.301620\t1.519635\n",
            "3390\t1.304867\t1.516700\n",
            "3400\t1.267572\t1.515543\n",
            "3410\t1.286190\t1.504722\n",
            "3420\t1.286712\t1.517979\n",
            "3430\t1.246084\t1.506808\n",
            "3440\t1.258392\t1.497918\n",
            "3450\t1.273170\t1.507252\n",
            "3460\t1.310213\t1.507737\n",
            "3470\t1.239587\t1.522490\n",
            "3480\t1.286653\t1.525937\n",
            "3490\t1.246325\t1.517555\n",
            "3500\t1.264264\t1.513139\n",
            "3510\t1.315594\t1.515167\n",
            "3520\t1.253644\t1.513375\n",
            "3530\t1.250550\t1.515202\n",
            "3540\t1.307202\t1.508229\n",
            "3550\t1.280380\t1.511578\n",
            "3560\t1.266213\t1.506875\n",
            "3570\t1.236053\t1.518930\n",
            "3580\t1.340799\t1.513759\n",
            "3590\t1.254180\t1.515997\n",
            "3600\t1.279201\t1.507054\n",
            "3610\t1.341135\t1.509766\n",
            "3620\t1.284773\t1.509961\n",
            "3630\t1.240577\t1.517850\n",
            "3640\t1.314653\t1.516874\n",
            "3650\t1.313009\t1.514465\n",
            "3660\t1.242681\t1.519554\n",
            "3670\t1.260540\t1.515140\n",
            "3680\t1.246156\t1.513347\n",
            "3690\t1.263323\t1.506898\n",
            "3700\t1.250252\t1.506617\n",
            "3710\t1.249482\t1.504467\n",
            "3720\t1.271516\t1.504306\n",
            "3730\t1.281797\t1.513012\n",
            "3740\t1.289230\t1.512753\n",
            "3750\t1.260813\t1.520245\n",
            "3760\t1.292713\t1.507185\n",
            "3770\t1.244711\t1.516294\n",
            "3780\t1.261179\t1.507320\n",
            "3790\t1.348750\t1.509417\n",
            "3800\t1.296754\t1.505133\n",
            "3810\t1.280554\t1.509807\n",
            "3820\t1.287822\t1.511457\n",
            "3830\t1.272456\t1.504128\n",
            "3840\t1.220963\t1.504407\n",
            "3850\t1.253016\t1.506974\n",
            "3860\t1.297051\t1.501161\n",
            "3870\t1.241606\t1.507265\n",
            "3880\t1.283353\t1.504188\n",
            "3890\t1.289695\t1.504679\n",
            "3900\t1.232107\t1.508070\n",
            "3910\t1.285577\t1.503356\n",
            "3920\t1.275760\t1.502267\n",
            "3930\t1.227113\t1.507356\n",
            "3940\t1.290713\t1.504335\n",
            "3950\t1.263913\t1.507162\n",
            "3960\t1.268488\t1.510300\n",
            "3970\t1.267463\t1.495703\n",
            "3980\t1.289308\t1.504967\n",
            "3990\t1.217727\t1.499946\n",
            "4000\t1.265716\t1.500250\n",
            "4010\t1.257322\t1.506407\n",
            "4020\t1.265836\t1.502662\n",
            "4030\t1.260200\t1.503690\n",
            "4040\t1.260423\t1.501222\n",
            "4050\t1.285199\t1.498170\n",
            "4060\t1.242649\t1.498450\n",
            "4070\t1.258749\t1.506052\n",
            "4080\t1.275464\t1.508249\n",
            "4090\t1.266686\t1.499470\n",
            "4100\t1.278874\t1.504208\n",
            "4110\t1.240087\t1.499697\n",
            "4120\t1.246829\t1.498394\n",
            "4130\t1.247843\t1.495229\n",
            "4140\t1.267753\t1.504374\n",
            "4150\t1.253618\t1.493015\n",
            "4160\t1.251896\t1.491284\n",
            "4170\t1.308493\t1.492825\n",
            "4180\t1.229135\t1.500452\n",
            "4190\t1.258784\t1.505010\n",
            "4200\t1.227214\t1.500123\n",
            "4210\t1.256627\t1.506246\n",
            "4220\t1.234984\t1.504765\n",
            "4230\t1.187345\t1.497707\n",
            "4240\t1.199781\t1.495366\n",
            "4250\t1.223228\t1.502407\n",
            "4260\t1.259613\t1.507906\n",
            "4270\t1.262957\t1.503417\n",
            "4280\t1.265766\t1.504884\n",
            "4290\t1.253227\t1.507322\n",
            "4300\t1.232918\t1.506543\n",
            "4310\t1.260496\t1.504562\n",
            "4320\t1.206661\t1.507352\n",
            "4330\t1.263990\t1.509871\n",
            "4340\t1.224412\t1.509950\n",
            "4350\t1.279120\t1.509629\n",
            "4360\t1.213720\t1.503927\n",
            "4370\t1.274205\t1.521494\n",
            "4380\t1.295801\t1.504715\n",
            "4390\t1.237134\t1.508972\n",
            "4400\t1.244378\t1.504158\n",
            "4410\t1.253127\t1.500964\n",
            "4420\t1.288256\t1.508305\n",
            "4430\t1.310556\t1.498472\n",
            "4440\t1.239033\t1.503729\n",
            "4450\t1.203153\t1.505268\n",
            "4460\t1.259257\t1.498318\n",
            "4470\t1.206266\t1.497124\n",
            "4480\t1.236713\t1.500193\n",
            "4490\t1.196070\t1.488997\n",
            "4500\t1.209896\t1.501532\n",
            "4510\t1.160611\t1.508732\n",
            "4520\t1.186358\t1.491094\n",
            "4530\t1.263304\t1.502524\n",
            "4540\t1.256449\t1.502612\n",
            "4550\t1.246720\t1.496384\n",
            "4560\t1.254256\t1.499526\n",
            "4570\t1.242534\t1.504996\n",
            "4580\t1.204839\t1.492934\n",
            "4590\t1.230418\t1.491621\n",
            "4600\t1.225282\t1.503972\n",
            "4610\t1.272506\t1.501485\n",
            "4620\t1.234215\t1.500421\n",
            "4630\t1.259325\t1.498705\n",
            "4640\t1.239917\t1.508726\n",
            "4650\t1.223493\t1.510522\n",
            "4660\t1.274123\t1.513041\n",
            "4670\t1.215264\t1.502798\n",
            "4680\t1.252210\t1.508489\n",
            "4690\t1.227178\t1.496092\n",
            "4700\t1.222137\t1.497484\n",
            "4710\t1.216894\t1.501744\n",
            "4720\t1.214582\t1.504817\n",
            "4730\t1.221695\t1.506587\n",
            "4740\t1.212589\t1.496639\n",
            "4750\t1.238801\t1.502587\n",
            "4760\t1.224868\t1.500093\n",
            "4770\t1.220975\t1.500769\n",
            "4780\t1.222590\t1.499557\n",
            "4790\t1.217426\t1.493679\n",
            "4800\t1.218578\t1.493052\n",
            "4810\t1.261418\t1.493573\n",
            "4820\t1.216406\t1.507368\n",
            "4830\t1.247225\t1.504171\n",
            "4840\t1.229038\t1.499397\n",
            "4850\t1.272891\t1.495621\n",
            "4860\t1.210048\t1.494681\n",
            "4870\t1.189166\t1.494542\n",
            "4880\t1.213595\t1.497516\n",
            "4890\t1.234130\t1.497143\n",
            "4900\t1.222410\t1.499670\n",
            "4910\t1.216767\t1.492500\n",
            "4920\t1.185298\t1.499614\n",
            "4930\t1.215339\t1.495978\n",
            "4940\t1.237110\t1.499074\n",
            "4950\t1.205427\t1.494531\n",
            "4960\t1.231066\t1.490048\n",
            "4970\t1.212784\t1.497188\n",
            "4980\t1.162270\t1.494365\n",
            "4990\t1.172724\t1.500872\n",
            "CPU times: user 2min 41s, sys: 853 ms, total: 2min 42s\n",
            "Wall time: 2min 44s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "torch.set_float32_matmul_precision('medium')\n",
        "for step in range(5000):\n",
        "  x, y = get_batch(batch_size=32, seq_length=128)\n",
        "  with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
        "    model.train()\n",
        "    logits = model.forward(x)\n",
        "    loss = loss_fun(logits.permute(0, 2, 1), y.long())\n",
        "\n",
        "  scaler.scale(loss).backward()\n",
        "  scaler.step(opt)\n",
        "  scaler.update()\n",
        "  # loss.backward()\n",
        "  # opt.step()\n",
        "  # opt.zero_grad()\n",
        "\n",
        "  if step % 10 == 0:\n",
        "    with torch.no_grad():\n",
        "      with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
        "        model.eval()\n",
        "        vlogits = model.forward(xv)\n",
        "        vloss = loss_fun(vlogits.permute(0,2,1), yv.long())\n",
        "      print('%d\\t%f\\t%f'%(step, loss, vloss))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.profiler.profile(with_stack=True, profile_memory=True) as prof:\n",
        "  x, y = get_batch(batch_size=32, seq_length=128)\n",
        "  with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
        "    model.train()\n",
        "    logits = model.forward(x)\n",
        "    loss = loss_fun(logits.permute(0, 2, 1), y.long())\n",
        "\n",
        "  loss.backward()\n",
        "  opt.step()\n",
        "  opt.zero_grad()\n",
        "\n",
        "\n",
        "print(prof.export_chrome_trace('trace3.json'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrLev-eo-yVi",
        "outputId": "deb056b5-f8ca-4181-d928-837b4aa769a9"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh trace2.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27DituDcAbN9",
        "outputId": "d78deaa3-a09c-4437-c7b3-0779f9748abf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 6.2M Oct 31 15:41 trace2.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3owgLEWXeJI",
        "outputId": "d4b2ae8b-52e7-4415-ab64-24e0839fae4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-e743fabf5d0e>:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  idx = torch.tensor(idx, device=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To dream to castle,\n",
            "And then let the Thortands o' the conquerors,\n",
            "Orgiving them speaking on the temper, and kell'd the breast,\n",
            "Whicomes fouly, caratuly omakin-cowagininininin: ninas omay Sas honine lhamebaponin ininedes ulys, tyines I pavininoraperes ty te'sarines\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "  with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
        "    model.eval()\n",
        "    print(decode(model.generate(encode('To dream'), 256)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqWfDKngMlVJ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(model.embeddings.weight.detach().cpu().numpy())\n",
        "# plt.plot(model.pos_embeddings.weight[:,37].detach().cpu().numpy())\n",
        "# plt.plot(model.pos_embeddings.weight[:,4].detach().cpu().numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sn9Q42SwPPpT"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "tsne = TSNE()\n",
        "y = tsne.fit_transform(model.out.weight.detach().cpu().numpy())\n",
        "\n",
        "plt.scatter(y[:,0], y[:,1], s=1, alpha=0.1)\n",
        "for i, c in enumerate(chars):\n",
        "  plt.text(y[i,0], y[i,1], c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfCg1bDIQblk"
      },
      "outputs": [],
      "source": [
        "model.out.weight.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xlc_aDPnPmLK"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMK0KBuPauF685J8dRunswD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNBvhqNNnRZXr9osQqUhr1Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/steelannelida/nanoGPT/blob/master/nanogpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/steelannelida/nanoGPT.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtaaN1C1BT8P",
        "outputId": "c8c4f80c-bcfe-4f48-b6a1-5755649a33f2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nanoGPT'...\n",
            "remote: Enumerating objects: 691, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 691 (delta 4), reused 0 (delta 0), pack-reused 682 (from 1)\u001b[K\n",
            "Receiving objects: 100% (691/691), 961.34 KiB | 17.48 MiB/s, done.\n",
            "Resolving deltas: 100% (389/389), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch numpy transformers datasets tiktoken wandb tqdm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vq-zNSZYCPTL",
        "outputId": "0a3b208e-ff20-4d28-961b-677edd4a4cac"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.0.2-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.17.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Downloading datasets-3.0.2-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.7/472.7 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, tiktoken, multiprocess, datasets\n",
            "Successfully installed datasets-3.0.2 dill-0.3.8 multiprocess-0.70.16 tiktoken-0.8.0 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!yes  | python nanoGPT/data/shakespeare/prepare.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSPqPSIgCAGI",
        "outputId": "0b26085f-e0a3-42ed-c45e-ef01bffb9f51"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train has 301,966 tokens\n",
            "val has 36,059 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vziHkeCMFLt8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('nanoGPT/data/shakespeare/input.txt') as f:\n",
        "  text = f.read()\n",
        "\n",
        "chars = sorted(set(text))\n",
        "vocab_size=len(chars)\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "\n",
        "def encode(t):\n",
        "  return [stoi[c] for c in t]\n",
        "\n",
        "def decode(seq):\n",
        "  return ''.join([chars[i] for i in seq])\n",
        "\n",
        "decode(encode(\"sandwitch\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CDskvzaJC2dL",
        "outputId": "6a69759b-6d04-45db-ecfc-6d7e0e1adccb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sandwitch'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "data = torch.tensor(encode(text), device=device)\n",
        "data.shape, data.dtype\n",
        "\n",
        "n = int(data.shape[0] * 0.9)\n",
        "train_data = data[:n]\n",
        "valid_data = data[n:]"
      ],
      "metadata": {
        "id": "emtQzoojEXUI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.manual_seed(1337)\n",
        "\n",
        "\n",
        "def get_batch(data_set=train_data, batch_size=32, seq_length=128):\n",
        "    x = torch.zeros([batch_size, seq_length], dtype=torch.int, device=device)\n",
        "    y = torch.zeros([batch_size, seq_length], dtype=torch.int, device=device)\n",
        "    for b in range(batch_size):\n",
        "        t = torch.randint(0, data_set.shape[0] - seq_length - 1, [1])\n",
        "        x[b] = data_set[t:t+seq_length]\n",
        "        y[b] = data_set[t+1:t+seq_length+1]\n",
        "    return x, y\n",
        "\n",
        "x, y = get_batch()\n",
        "print(decode(x[13]))\n",
        "print(decode(y[13]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3O_mdTCJyKW",
        "outputId": "2f34c928-847c-4180-c9ec-dfb35fbeac1c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S:\n",
            "Let him know't.\n",
            "\n",
            "FLORIZEL:\n",
            "He shall not.\n",
            "\n",
            "POLIXENES:\n",
            "Prithee, let him.\n",
            "\n",
            "FLORIZEL:\n",
            "No, he must not.\n",
            "\n",
            "Shepherd:\n",
            "Let him, my son\n",
            ":\n",
            "Let him know't.\n",
            "\n",
            "FLORIZEL:\n",
            "He shall not.\n",
            "\n",
            "POLIXENES:\n",
            "Prithee, let him.\n",
            "\n",
            "FLORIZEL:\n",
            "No, he must not.\n",
            "\n",
            "Shepherd:\n",
            "Let him, my son:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self, embed_size=256, nheads=16, broad_size=1024):\n",
        "    super.__init__()\n",
        "    self.attn = nn.MultiheadAttention(embed_size, nheads, batch_first=True)\n",
        "    self.ffwd1 = nn.Linear(embed_size, broad_size)\n",
        "    self.ffwd2 = nn.Linear(broad_size, embed_size)\n",
        "\n",
        "  def forward(self, embeds, mask):\n",
        "    nn.functional.layer_norm(embeds)\n",
        "\n",
        "\n",
        "class LM(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_size=256, nheads=16, max_pos=2048, num_layers=3):\n",
        "    super().__init__()\n",
        "    self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
        "    self.pos_embeddings = nn.Embedding(max_pos, embed_size)\n",
        "    self.attns = [\n",
        "        nn.MultiheadAttention(embed_size, nheads, batch_first=True)\n",
        "        for _ in range(num_layers)\n",
        "    ]\n",
        "    for i, attn in enumerate(self.attns):\n",
        "      self.add_module(f'attn-{i}', attn)\n",
        "    self.out = nn.Linear(embed_size, vocab_size)\n",
        "    self.max_pos = max_pos\n",
        "    self.to(device)\n",
        "\n",
        "  def forward(self, idx):\n",
        "    idx = torch.tensor(idx, device=device)\n",
        "    l = idx.shape[-1]\n",
        "    pe = self.pos_embeddings(torch.arange(0, l, device=device))\n",
        "    e = self.embeddings(idx)\n",
        "    e = e + pe.view(1, *pe.shape)\n",
        "    mask = torch.tril(torch.ones([l, l], device=device)).T\n",
        "    #print(mask)\n",
        "    for attn in self.attns:\n",
        "      a, w = attn.forward(e, e, e, attn_mask=mask)\n",
        "      e = e + a\n",
        "    logits = self.out.forward(e)\n",
        "    return logits\n",
        "\n",
        "  def generate(self, prompt, l):\n",
        "    prompt = torch.tensor(prompt)\n",
        "    pl = prompt.shape[0]\n",
        "    result = torch.zeros([pl + l], dtype=torch.int, device=device)\n",
        "    result[:pl] = prompt\n",
        "    for i in range(l):\n",
        "      logits = self.forward(result[:pl + i])\n",
        "      sm = logits[:,-1].flatten().softmax(0)\n",
        "      next_idx = torch.multinomial(sm, 1)\n",
        "      result[i+pl] = next_idx\n",
        "    return result\n",
        "\n",
        "model = LM(vocab_size)\n",
        "\n",
        "k=x\n",
        "q=x[:,:-1]\n",
        "model.forward(encode('jello'))\n",
        "decode(model.generate(encode('hello'), 10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "d6Huvb7cNXcN",
        "outputId": "5168aabc-c5b2-4d24-a484-ec1441453740"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-e1028aa98984>:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  idx = torch.tensor(idx, device=device)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'helloHRrx,tsmI&'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = get_batch()\n",
        "logits = model(x)\n",
        "loss_fun = nn.CrossEntropyLoss()\n",
        "loss = loss_fun(logits.permute(0, 2, 1), y.long())\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmqqz6egO50w",
        "outputId": "e7837753-ef97-433e-90f6-4fc55226af9f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-e1028aa98984>:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  idx = torch.tensor(idx, device=device)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.5210, device='cuda:0', grad_fn=<NllLoss2DBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "model = LM(vocab_size)\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "xv, yv = get_batch(valid_data, batch_size=4)\n"
      ],
      "metadata": {
        "id": "0nkmynipacvK"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "for step in range(2000):\n",
        "  x, y = get_batch(batch_size=64)\n",
        "  model.train()\n",
        "  logits = model.forward(x)\n",
        "  loss = loss_fun(logits.permute(0, 2, 1), y.long())\n",
        "  opt.zero_grad()\n",
        "  loss.backward()\n",
        "  opt.step()\n",
        "\n",
        "  if step % 100 == 0:\n",
        "    model.eval()\n",
        "    vlogits = model.forward(xv)\n",
        "    vloss = loss_fun(vlogits.permute(0,2,1), yv.long())\n",
        "    print('%d\\t%f\\t%f'%(step, loss, vloss))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5yC7g_TUpZz",
        "outputId": "8c7ae326-8a07-48ee-e027-d2251efaf7ce"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-e1028aa98984>:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  idx = torch.tensor(idx, device=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\t0.016441\t0.017914\n",
            "100\t0.015795\t0.015420\n",
            "200\t0.018466\t0.015636\n",
            "300\t0.016612\t0.017842\n",
            "400\t0.016058\t0.025138\n",
            "500\t0.016307\t0.019679\n",
            "600\t0.015800\t0.017379\n",
            "700\t0.017254\t0.018920\n",
            "800\t0.014998\t0.018186\n",
            "900\t0.018116\t0.020165\n",
            "1000\t0.016869\t0.016565\n",
            "1100\t0.016267\t0.021569\n",
            "1200\t0.016420\t0.017768\n",
            "1300\t0.016539\t0.019839\n",
            "1400\t0.017085\t0.021222\n",
            "1500\t0.020328\t0.022157\n",
            "1600\t0.015081\t0.019696\n",
            "1700\t0.020653\t0.024213\n",
            "1800\t0.017644\t0.017772\n",
            "1900\t0.015347\t0.016363\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(x[0]))"
      ],
      "metadata": {
        "id": "egeAnUUfXsNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(model.generate(encode('Hello!'), 120)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3owgLEWXeJI",
        "outputId": "964fb729-1acc-4910-b102-187373d4bea9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-e1028aa98984>:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  idx = torch.tensor(idx, device=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello!!l!!;UMPUUUq!!UUMPUERUU&!.BUDUDUKUKUMPUUUHUKUqUMOUUUUDUUENUMwUMU ! ! qJU!!UTUJUFUU!UDUjUUJUBE!QUBUUKUBOUFDUJUJUOUUMUUMUO\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Xlc_aDPnPmLK"
      }
    }
  ]
}
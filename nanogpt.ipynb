{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMctOhfcgSWwg2qtvJNdhyT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/steelannelida/nanoGPT/blob/master/nanogpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/steelannelida/nanoGPT.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtaaN1C1BT8P",
        "outputId": "c60d0a98-c934-4571-8bed-d6e9fdd89468"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'nanoGPT' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch numpy transformers datasets tiktoken wandb tqdm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vq-zNSZYCPTL",
        "outputId": "4a7ea90d-001d-45b2-b6be-d6f10889cf67"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.17.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!yes  | python nanoGPT/data/shakespeare/prepare.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSPqPSIgCAGI",
        "outputId": "2ed9b955-9a20-4dd0-9157-e853c5dc4420"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train has 301,966 tokens\n",
            "val has 36,059 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vziHkeCMFLt8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('nanoGPT/data/shakespeare/input.txt') as f:\n",
        "  text = f.read()\n",
        "\n",
        "chars = sorted(set(text))\n",
        "vocab_size=len(chars)\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "\n",
        "def encode(t):\n",
        "  return [stoi[c] for c in t]\n",
        "\n",
        "def decode(seq):\n",
        "  return ''.join([chars[i] for i in seq])\n",
        "\n",
        "decode(encode(\"sandwitch\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CDskvzaJC2dL",
        "outputId": "f3471bad-7529-45e2-f2d0-5224fa26a927"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sandwitch'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "data = torch.tensor(encode(text), device=device)\n",
        "data.shape, data.dtype\n",
        "\n",
        "n = int(data.shape[0] * 0.9)\n",
        "train_data = data[:n]\n",
        "valid_data = data[n:]"
      ],
      "metadata": {
        "id": "emtQzoojEXUI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.manual_seed(1337)\n",
        "\n",
        "\n",
        "def get_batch(data_set=train_data, batch_size=32, seq_length=128):\n",
        "    x = torch.zeros([batch_size, seq_length], dtype=torch.int, device=device)\n",
        "    y = torch.zeros([batch_size, seq_length], dtype=torch.int, device=device)\n",
        "    for b in range(batch_size):\n",
        "        t = torch.randint(0, data_set.shape[0] - seq_length - 1, [1])\n",
        "        x[b] = data_set[t:t+seq_length]\n",
        "        y[b] = data_set[t+1:t+seq_length+1]\n",
        "    return x, y\n",
        "\n",
        "x, y = get_batch()\n",
        "print(decode(x[13]))\n",
        "print(decode(y[13]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3O_mdTCJyKW",
        "outputId": "eec2caf1-c15b-4c68-a6ee-b9e2efd6f41c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "e of me\n",
            "And stole into the covert of the wood:\n",
            "I, measuring his affections by my own,\n",
            "That most are busied when they're most alo\n",
            " of me\n",
            "And stole into the covert of the wood:\n",
            "I, measuring his affections by my own,\n",
            "That most are busied when they're most alon\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self, embed_size=256, wide_size=1024, nheads=16, dropout=0.5):\n",
        "    super().__init__()\n",
        "    self.attn = nn.MultiheadAttention(embed_size, nheads,\n",
        "                                      batch_first=True)\n",
        "    self.ln1 = nn.LayerNorm(embed_size)\n",
        "    self.ln2 = nn.LayerNorm(embed_size)\n",
        "    self.lin1 = nn.Linear(embed_size, wide_size)\n",
        "    self.lin2 = nn.Linear(wide_size, embed_size)\n",
        "    self.drop = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    x = self.ln1(x)\n",
        "    a, w = self.attn.forward(x, x, x, attn_mask=mask, is_causal=True)\n",
        "    x = x + self.drop(a)\n",
        "    x = self.ln2(x)\n",
        "    xx = self.lin1(x)\n",
        "    xx = nn.functional.gelu(xx)\n",
        "    y = self.lin2(xx)\n",
        "    return x + y\n",
        "\n",
        "\n",
        "\n",
        "class LM(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_size=256, nheads=16, max_pos=2048, num_layers=3, dropout=0.5):\n",
        "    super().__init__()\n",
        "    self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
        "    self.pos_embeddings = nn.Embedding(max_pos, embed_size)\n",
        "    self.layers = [\n",
        "        DecoderLayer(embed_size=embed_size, wide_size=4 * embed_size,\n",
        "                     nheads=nheads, dropout=dropout)\n",
        "        for _ in range(num_layers)\n",
        "    ]\n",
        "    for i, layer in enumerate(self.layers):\n",
        "      self.add_module(f'decoder-{i}', layer)\n",
        "    self.ln = nn.LayerNorm(embed_size)\n",
        "    self.out = nn.Linear(embed_size, vocab_size)\n",
        "    self.max_pos = max_pos\n",
        "    self.to(device)\n",
        "\n",
        "  def forward(self, idx):\n",
        "    idx = torch.tensor(idx, device=device)\n",
        "    l = idx.shape[-1]\n",
        "    pe = self.pos_embeddings(torch.arange(0, l, device=device))\n",
        "    e = self.embeddings(idx)\n",
        "    e = e + pe.view(1, *pe.shape)\n",
        "    mask = ~torch.tril(torch.ones([l, l], dtype=torch.bool, device=device))\n",
        "    #print(mask)\n",
        "    #print(w)\n",
        "    for layer in self.layers:\n",
        "      e = layer(e, mask)\n",
        "    logits = self.out.forward(self.ln(e))\n",
        "    return logits\n",
        "\n",
        "  def generate(self, prompt, l):\n",
        "    prompt = torch.tensor(prompt)\n",
        "    pl = prompt.shape[0]\n",
        "    result = torch.zeros([pl + l], dtype=torch.int, device=device)\n",
        "    result[:pl] = prompt\n",
        "    for i in range(l):\n",
        "      logits = self.forward(result[:pl + i])\n",
        "      sm = logits[:,-1].flatten().softmax(0)\n",
        "      next_idx = torch.multinomial(sm, 1)\n",
        "      result[i+pl] = next_idx\n",
        "    return result\n",
        "\n",
        "model = LM(vocab_size)\n",
        "\n",
        "k=x\n",
        "q=x[:,:-1]\n",
        "model.forward(encode('jello'))\n",
        "decode(model.generate(encode('hello'), 10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "d6Huvb7cNXcN",
        "outputId": "e16a5ff4-ff02-4fd4-efbe-b6b643168ac2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-e0d01c3d5bba>:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  idx = torch.tensor(idx, device=device)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hellorgSbmFEw&t'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = get_batch()\n",
        "logits = model(x)\n",
        "loss_fun = nn.CrossEntropyLoss()\n",
        "loss = loss_fun(logits.permute(0, 2, 1), y.long())\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmqqz6egO50w",
        "outputId": "492f258b-ca5e-4f82-ca95-04d58f347287"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-e0d01c3d5bba>:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  idx = torch.tensor(idx, device=device)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.3257, device='cuda:0', grad_fn=<NllLoss2DBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "model = LM(vocab_size)\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "xv, yv = get_batch(valid_data, batch_size=256)\n"
      ],
      "metadata": {
        "id": "0nkmynipacvK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for step in range(5000):\n",
        "  x, y = get_batch(batch_size=64, seq_length=512)\n",
        "  model.train()\n",
        "  logits = model.forward(x)\n",
        "  loss = loss_fun(logits.permute(0, 2, 1), y.long())\n",
        "  opt.zero_grad()\n",
        "  loss.backward()\n",
        "  opt.step()\n",
        "\n",
        "  if step % 100 == 0:\n",
        "    model.eval()\n",
        "    vlogits = model.forward(xv)\n",
        "    vloss = loss_fun(vlogits.permute(0,2,1), yv.long())\n",
        "    print('%d\\t%f\\t%f'%(step, loss, vloss))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5yC7g_TUpZz",
        "outputId": "99733d21-39e6-453a-fd40-eb8b21d6de1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-e0d01c3d5bba>:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  idx = torch.tensor(idx, device=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\t4.324728\t3.536380\n",
            "100\t2.458360\t2.467422\n",
            "200\t2.402823\t2.390305\n",
            "300\t2.336194\t2.327637\n",
            "400\t2.226765\t2.207525\n",
            "500\t2.068967\t2.138275\n",
            "600\t1.970387\t2.076533\n",
            "700\t1.830615\t1.955126\n",
            "800\t1.742913\t1.879662\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "print(decode(model.generate(encode('To dream'), 256)))"
      ],
      "metadata": {
        "id": "t3owgLEWXeJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Xlc_aDPnPmLK"
      }
    }
  ]
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/steelannelida/nanoGPT/blob/master/nanogpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtaaN1C1BT8P",
        "outputId": "70b90e6f-1869-4166-e571-5eef9f86cacf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nanoGPT'...\n",
            "remote: Enumerating objects: 700, done.\u001b[K\n",
            "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 700 (delta 10), reused 10 (delta 6), pack-reused 682 (from 1)\u001b[K\n",
            "Receiving objects: 100% (700/700), 1.10 MiB | 20.43 MiB/s, done.\n",
            "Resolving deltas: 100% (395/395), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/steelannelida/nanoGPT.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vq-zNSZYCPTL",
        "outputId": "e31a6f24-5a30-4b2a-a434-05f3509839d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.0.2-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.17.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Downloading datasets-3.0.2-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.7/472.7 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, tiktoken, multiprocess, datasets\n",
            "Successfully installed datasets-3.0.2 dill-0.3.8 multiprocess-0.70.16 tiktoken-0.8.0 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch numpy transformers datasets tiktoken wandb tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSPqPSIgCAGI",
        "outputId": "af40e1d2-60c4-4067-eaca-0a9d67cf34b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train has 301,966 tokens\n",
            "val has 36,059 tokens\n"
          ]
        }
      ],
      "source": [
        "!yes  | python nanoGPT/data/shakespeare/prepare.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vziHkeCMFLt8"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CDskvzaJC2dL",
        "outputId": "51670260-aef4-4de4-8bfe-06da4aa96a55"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sandwitch'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "with open('nanoGPT/data/shakespeare/input.txt') as f:\n",
        "  text = f.read()\n",
        "\n",
        "chars = sorted(set(text))\n",
        "vocab_size=len(chars)\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "\n",
        "def encode(t):\n",
        "  return [stoi[c] for c in t]\n",
        "\n",
        "def decode(seq):\n",
        "  return ''.join([chars[i] for i in seq])\n",
        "\n",
        "decode(encode(\"sandwitch\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "emtQzoojEXUI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "data = torch.tensor(encode(text), device=device)\n",
        "data.shape, data.dtype\n",
        "\n",
        "n = int(data.shape[0] * 0.9)\n",
        "train_data = data[:n]\n",
        "valid_data = data[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3O_mdTCJyKW",
        "outputId": "16932e5c-1b83-4aaf-cb2b-25663f449700"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "l news at hand:\n",
            "My bosom's lord sits lightly in his throne;\n",
            "And all this day an unaccustom'd spirit\n",
            "Lifts me above the ground wi\n",
            " news at hand:\n",
            "My bosom's lord sits lightly in his throne;\n",
            "And all this day an unaccustom'd spirit\n",
            "Lifts me above the ground wit\n"
          ]
        }
      ],
      "source": [
        "#torch.manual_seed(1337)\n",
        "\n",
        "\n",
        "def get_batch(data_set=train_data, batch_size=32, seq_length=128):\n",
        "    x = torch.zeros([batch_size, seq_length], dtype=torch.int, device=device)\n",
        "    y = torch.zeros([batch_size, seq_length], dtype=torch.int, device=device)\n",
        "    for b in range(batch_size):\n",
        "        t = torch.randint(0, data_set.shape[0] - seq_length - 1, [1])\n",
        "        x[b] = data_set[t:t+seq_length]\n",
        "        y[b] = data_set[t+1:t+seq_length+1]\n",
        "    return x, y\n",
        "\n",
        "x, y = get_batch()\n",
        "print(decode(x[13]))\n",
        "print(decode(y[13]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Q8L2PQYERYsM",
        "outputId": "f855c88a-6dc6-4829-b495-09214af6533f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'70059-41345*43069=-1780617746'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import random\n",
        "arith_chars = '0123456789-=*+'\n",
        "arith_idx = {c:idx for idx, c in enumerate(arith_chars)}\n",
        "\n",
        "\n",
        "def gen_expression(digits=5, terms=3):\n",
        "  s = ''\n",
        "  for i in range(terms):\n",
        "    if i > 0:\n",
        "      s += random.choice('*-+')\n",
        "    num = ''\n",
        "    for j in range(digits):\n",
        "      num += random.choice('0123456789')\n",
        "    num = str(int(num))\n",
        "    s += num\n",
        "  return s\n",
        "\n",
        "def gen_eq():\n",
        "  expr = gen_expression()\n",
        "  value = eval(expr)\n",
        "  return f'{expr}={value}'\n",
        "\n",
        "gen_eq()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "d6Huvb7cNXcN",
        "outputId": "42c0531f-be26-4222-8163-45723e3b863f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-46e458458b13>:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  idx = torch.tensor(idx, device=device)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hellomblgYonD!M'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self, embed_size=256, wide_size=1024, nheads=16, dropout=0.5):\n",
        "    super().__init__()\n",
        "    self.attn = nn.MultiheadAttention(embed_size, nheads,\n",
        "                                      batch_first=True)\n",
        "    self.ln1 = nn.LayerNorm(embed_size)\n",
        "    self.ln2 = nn.LayerNorm(embed_size)\n",
        "    self.lin1 = nn.Linear(embed_size, wide_size)\n",
        "    self.lin2 = nn.Linear(wide_size, embed_size)\n",
        "    self.drop = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    x = self.ln1(x)\n",
        "    a, w = self.attn.forward(x, x, x, attn_mask=mask, is_causal=True)\n",
        "    x = x + self.drop(a)\n",
        "    x = self.ln2(x)\n",
        "    xx = self.lin1(x)\n",
        "    xx = nn.functional.gelu(xx)\n",
        "    y = self.lin2(xx)\n",
        "    return x + y\n",
        "\n",
        "\n",
        "\n",
        "class LM(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_size=256, nheads=16, max_pos=2048, num_layers=3, dropout=0.5):\n",
        "    super().__init__()\n",
        "    self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
        "    self.pos_embeddings = nn.Embedding(max_pos, embed_size)\n",
        "    self.layers = [\n",
        "        DecoderLayer(embed_size=embed_size, wide_size=4 * embed_size,\n",
        "                     nheads=nheads, dropout=dropout)\n",
        "        for _ in range(num_layers)\n",
        "    ]\n",
        "    for i, layer in enumerate(self.layers):\n",
        "      self.add_module(f'decoder-{i}', layer)\n",
        "    self.ln = nn.LayerNorm(embed_size)\n",
        "    self.out = nn.Linear(embed_size, vocab_size)\n",
        "    self.max_pos = max_pos\n",
        "    self.to(device)\n",
        "\n",
        "  def forward(self, idx):\n",
        "    idx = torch.tensor(idx, device=device)\n",
        "    l = idx.shape[-1]\n",
        "    pe = self.pos_embeddings(torch.arange(0, l, device=device))\n",
        "    e = self.embeddings(idx)\n",
        "    e = e + pe.view(1, *pe.shape)\n",
        "    mask = ~torch.tril(torch.ones([l, l], dtype=torch.bool, device=device))\n",
        "    #print(mask)\n",
        "    #print(w)\n",
        "    for layer in self.layers:\n",
        "      e = layer(e, mask)\n",
        "    logits = self.out.forward(self.ln(e))\n",
        "    return logits\n",
        "\n",
        "  def generate(self, prompt, l):\n",
        "    prompt = torch.tensor(prompt)\n",
        "    pl = prompt.shape[0]\n",
        "    result = torch.zeros([pl + l], dtype=torch.int, device=device)\n",
        "    result[:pl] = prompt\n",
        "    for i in range(l):\n",
        "      logits = self.forward(result[:pl + i])\n",
        "      sm = logits[:,-1].flatten().softmax(0)\n",
        "      next_idx = torch.multinomial(sm, 1)\n",
        "      result[i+pl] = next_idx\n",
        "    return result\n",
        "\n",
        "model = LM(vocab_size)\n",
        "\n",
        "k=x\n",
        "q=x[:,:-1]\n",
        "model.forward(encode('jello'))\n",
        "decode(model.generate(encode('hello'), 10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmqqz6egO50w",
        "outputId": "67b79c8a-ef09-42e6-f862-7e1f8d70ba15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-46e458458b13>:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  idx = torch.tensor(idx, device=device)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.2949, device='cuda:0', grad_fn=<NllLoss2DBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "x, y = get_batch()\n",
        "logits = model(x)\n",
        "loss_fun = nn.CrossEntropyLoss()\n",
        "loss = loss_fun(logits.permute(0, 2, 1), y.long())\n",
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nkmynipacvK",
        "outputId": "cabbbb4f-df52-4bd5-8c41-a454ea6871eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-68af09e1e786>:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "model = LM(vocab_size, num_layers=8)\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "xv, yv = get_batch(valid_data, batch_size=256)\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5yC7g_TUpZz",
        "outputId": "341e1098-b711-4eae-91f3-d267e6088f56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-46e458458b13>:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  idx = torch.tensor(idx, device=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\t2.124569\t2.153862\n",
            "10\t2.075121\t2.118199\n",
            "20\t2.077763\t2.122209\n",
            "30\t2.019752\t2.102204\n",
            "40\t2.061837\t2.096250\n",
            "50\t2.008243\t2.091561\n",
            "60\t2.057459\t2.079584\n",
            "70\t2.045761\t2.063578\n",
            "80\t2.027742\t2.054062\n",
            "90\t1.969361\t2.037554\n"
          ]
        }
      ],
      "source": [
        "torch.set_float32_matmul_precision('medium')\n",
        "with torch.profiler.profile(with_stack=True, profile_memory=True) as prof:\n",
        "  for step in range(10):\n",
        "    x, y = get_batch(batch_size=32, seq_length=128)\n",
        "    with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
        "      model.train()\n",
        "      logits = model.forward(x)\n",
        "      loss = loss_fun(logits.permute(0, 2, 1), y.long())\n",
        "\n",
        "    #scaler.scale(loss).backward()\n",
        "    #scaler.step(opt)\n",
        "    #scaler.update()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    opt.zero_grad()\n",
        "\n",
        "    if step % 10 == 0:\n",
        "      with torch.no_grad():\n",
        "        with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
        "          model.eval()\n",
        "          vlogits = model.forward(xv)\n",
        "          vloss = loss_fun(vlogits.permute(0,2,1), yv.long())\n",
        "        print('%d\\t%f\\t%f'%(step, loss, vloss))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(prof.export_chrome_trace('trace1.json'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrLev-eo-yVi",
        "outputId": "6f46bd99-f1cc-422d-ff85-1453cf61d04f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh trace1.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27DituDcAbN9",
        "outputId": "f6667fbe-f595-441e-af8c-397a08fc053a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 524M Oct 31 14:35 trace1.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3owgLEWXeJI",
        "outputId": "85816e59-934d-446b-f7eb-36825e9bc7fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-46e458458b13>:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  idx = torch.tensor(idx, device=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To dream revenge with my devil land Montagued\n",
            "When are here this living ere such wears escape\n",
            "For shall then place in thine own: prucacrumelde, fowese din.\n",
            "Wathe, hese thesers allay se the Juse bade, wbur rbear fo iethe warrurie mino ambon adld\n",
            "Nes sene rine, ouse\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "  with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
        "    model.eval()\n",
        "    print(decode(model.generate(encode('To dream'), 256)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqWfDKngMlVJ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(model.embeddings.weight.detach().cpu().numpy())\n",
        "# plt.plot(model.pos_embeddings.weight[:,37].detach().cpu().numpy())\n",
        "# plt.plot(model.pos_embeddings.weight[:,4].detach().cpu().numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sn9Q42SwPPpT"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "tsne = TSNE()\n",
        "y = tsne.fit_transform(model.out.weight.detach().cpu().numpy())\n",
        "\n",
        "plt.scatter(y[:,0], y[:,1], s=1, alpha=0.1)\n",
        "for i, c in enumerate(chars):\n",
        "  plt.text(y[i,0], y[i,1], c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfCg1bDIQblk"
      },
      "outputs": [],
      "source": [
        "model.out.weight.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xlc_aDPnPmLK"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMV/eYHnmPifINC7cMbd5a+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}